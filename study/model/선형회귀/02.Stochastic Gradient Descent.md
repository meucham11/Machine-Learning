```
앞선 Gradient Descent 보다 이걸 더 많이쓴다.
```
### Stochastic Gradient Descent
```
함수에서 한 점을 잡고 gradient를 구하는게 아니라
여러 지점에서 구해나간다.(함수가 구불구불할 때 유용하겠지, 지역최적화에 빠지지 않는다.)


업데이트가 빈번하여 모델 성능 및 개선 속도 확인 가능
일부 문제에 대해 더 빨리 수렴
지역 최적화 회피

대용량 데이터시 시간이 오래걸림
더 이상 cost가 줄어들지 않는 시점의 발견이 어려움
```
---
---

<img width="487" alt="캡처" src="https://user-images.githubusercontent.com/34879309/73618737-18392800-466d-11ea-8a1c-3476a3b3cb7f.PNG">
<img width="254" alt="캡처" src="https://user-images.githubusercontent.com/34879309/73618772-4e76a780-466d-11ea-9fc3-8cf3824d4aa8.PNG">

### full-batch gradient descent
```
한번에 데이터를 모두 넣는다.

업데이트 감소 -> 속도향상
안정적인 Cost 함수 수렴
지역 최적화 가능
메모리 문제,대규모 dataset -> 모델/파라메터 업데이트가 느려짐
```

---
---
## Mini-batch Stochastic Gradient Descent
```
x를 shuffle한다.
Stochastic Gradient Descent와 Batch Gradient의 혼합
한번의 일정량의 데이터를 랜덤하게 뽑아서 학습
가장 일반적으로 많이 쓰임
```

---
## 용어
### Epoch
```
전체 데이터가 training 데이터에 들어가는 횟수
full batch를 n번 실행하면 n epoch
```
### Batch size
```
전체 데이터는 1024개고
mini batch sgd를 할 때 자른 데이터가 128개라 치자
이 때 batch size는 128이고
128*8을 해야 1024 이므로 8번을 돌려야 1 epoch가 된다.

```


---
---
# Stochastic Gradient Descent 를 실제로 구현했을 때 생기는 여러 이슈
```
epoch를 몇번 할 것인지 (=iteration 되는 횟수)
: 위의 기법들마다 속도가 다르기 때문에 알아서 선택

learning rate를 일정하게 해야하는지
:목표지점에서 learning rate가 줄어들어야 촘촘하게 업데이트가 가능하다
learning rate decay를 쓴다. 일정한 주기로 learning rate를 감소시켜줌
```

