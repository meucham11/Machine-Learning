{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1,2,3,4,5]\n",
    "y_data = [1,2,3,4,5]\n",
    "\n",
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|    1.0048|  -0.01733|  0.000055\n",
      "   10|    1.0046|  -0.01675|  0.000051\n",
      "   20|    1.0045|  -0.01619|  0.000048\n",
      "   30|    1.0043|  -0.01566|  0.000045\n",
      "   40|    1.0042|  -0.01513|  0.000042\n",
      "   50|    1.0041|  -0.01463|  0.000039\n",
      "   60|    1.0039|  -0.01414|  0.000037\n",
      "   70|    1.0038|  -0.01367|  0.000034\n",
      "   80|    1.0037|  -0.01322|  0.000032\n",
      "   90|    1.0035|  -0.01278|  0.000030\n",
      "  100|    1.0034|  -0.01235|  0.000028\n",
      "  110|    1.0033|  -0.01194|  0.000026\n",
      "  120|    1.0032|  -0.01154|  0.000024\n",
      "  130|    1.0031|  -0.01116|  0.000023\n",
      "  140|    1.0030|  -0.01079|  0.000021\n",
      "  150|    1.0029|  -0.01043|  0.000020\n",
      "  160|    1.0028|  -0.01008|  0.000019\n",
      "  170|    1.0027| -0.009744|  0.000017\n",
      "  180|    1.0026|  -0.00942|  0.000016\n",
      "  190|    1.0025| -0.009106|  0.000015\n",
      "  200|    1.0024| -0.008803|  0.000014\n",
      "  210|    1.0024|  -0.00851|  0.000013\n",
      "  220|    1.0023| -0.008227|  0.000012\n",
      "  230|    1.0022| -0.007953|  0.000012\n",
      "  240|    1.0021| -0.007688|  0.000011\n",
      "  250|    1.0021| -0.007432|  0.000010\n",
      "  260|    1.0020| -0.007184|  0.000009\n",
      "  270|    1.0019| -0.006945|  0.000009\n",
      "  280|    1.0019| -0.006714|  0.000008\n",
      "  290|    1.0018|  -0.00649|  0.000008\n"
     ]
    }
   ],
   "source": [
    "learning_rate=tf.Variable(0.01) # step 크기\n",
    "\n",
    "for i in range(300):\n",
    "    with tf.GradientTape() as tape:\n",
    "        H = W * x_data + b\n",
    "        cost = tf.reduce_mean(tf.square(H - y_data))\n",
    "\n",
    "    W_grad, b_grad = tape.gradient(cost,[W,b])      # gradient함수는 첫번째 인자인 cost라는 함수에 대해 W와b 의 그레디언트 값을 반환한다.\n",
    "\n",
    "    W.assign_sub(learning_rate * W_grad)  # A.assign_sub(B)  =>  A = A-B 즉, A-=B.   계속 값을 업데이트\n",
    "    b.assign_sub(learning_rate * b_grad)  # 그라디언트값이 크면 좀더 많이 뛸 수 있다. 그래야 최소값에 빨리 도착할 수 있으니까!   \n",
    "    \n",
    "    if i % 10 ==0:\n",
    "        print(\"{:5}|{:10.4f}|{:10.4}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기값을 잘못 설정하게 되면, cost함수가 convex형이 아니라면 전체에서 W,b의 최소값을 찾을 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi_variable linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [73.,93.,89.,96.,73.]\n",
    "x2 = [80.,88.,91.,98.,66.]\n",
    "x3 = [75.,93.,90.,100.,70.]\n",
    "\n",
    "Y = [152.,185.,180.,196.,142.]\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([1]))\n",
    "w2 = tf.Variable(tf.random_normal([1]))\n",
    "w3 = tf.Variable(tf.random_normal([1]))\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|   1593.1958\n",
      "  100|      3.0903\n",
      "  200|      2.8917\n",
      "  300|      2.8889\n",
      "  400|      2.8862\n",
      "  500|      2.8835\n",
      "  600|      2.8808\n",
      "  700|      2.8781\n",
      "  800|      2.8754\n",
      "  900|      2.8727\n",
      " 1000|      2.8700\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.000001\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        H = w1*x1 + w2*x2 + w3*x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(H-Y))\n",
    "        \n",
    "    w1_grad,w2_grad,w3_grad,b_grad = tape.gradient(cost,[w1,w2,w3,b])\n",
    "\n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "    if i %100==0:\n",
    "        print(\"{:5}|{:12.4f}\".format(i,cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬로 구현해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 73.,  80.,  75., 152.],\n",
       "       [ 93.,  88.,  93., 185.],\n",
       "       [ 89.,  91.,  90., 180.],\n",
       "       [ 96.,  98., 100., 196.],\n",
       "       [ 73.,  66.,  70., 142.]], dtype=float32)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.transpose(np.array([x1,x2,x3,Y]))\n",
    "data = data.astype(\"float32\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:,:-1]\n",
    "Y = data[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([3,1]))\n",
    "b = tf.Variable(tf.random_normal([1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction func\n",
    "def hypothesis(X):\n",
    "    return tf.matmul(X,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|      7.7154\n",
      "  100|      7.6133\n",
      "  200|      7.5151\n",
      "  300|      7.4208\n",
      "  400|      7.3299\n",
      "  500|      7.2425\n",
      "  600|      7.1582\n",
      "  700|      7.0771\n",
      "  800|      6.9987\n",
      "  900|      6.9232\n",
      " 1000|      6.8503\n",
      " 1100|      6.7799\n",
      " 1200|      6.7117\n",
      " 1300|      6.6459\n",
      " 1400|      6.5821\n",
      " 1500|      6.5205\n",
      " 1600|      6.4607\n",
      " 1700|      6.4027\n",
      " 1800|      6.3464\n",
      " 1900|      6.2918\n",
      " 2000|      6.2388\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00001\n",
    "for i in range(2000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean((tf.square(hypothesis(X)-Y)))\n",
    "        \n",
    "    W_grad, b_grad = tape.gradient(cost,[W,b])\n",
    "\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "    if i %100==0:\n",
    "        print(\"{:5}|{:12.4f}\".format(i,cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.reduce_mean(Y * tf.log(H) - (1- Y)*tf.log(1-H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1., 2.],\n",
    "          [2., 3.],\n",
    "          [3., 1.],\n",
    "          [4., 3.],\n",
    "          [5., 3.],\n",
    "          [6., 2.]]\n",
    "y_train = [[0.],\n",
    "          [0.],\n",
    "          [0.],\n",
    "          [1.],\n",
    "          [1.],\n",
    "          [1.]]\n",
    "\n",
    "x_test = [[5.,2.]]\n",
    "y_test = [[1.]]\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))\n",
    "\n",
    "\n",
    "W = tf.Variable(tf.zeros([2,1]), name='weight')\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features):\n",
    "    H = tf.divide(1.,1.+tf.exp(tf.matmul(features, W)+b))\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(H, labels):\n",
    "    cost = -tf.reduce_mean(labels*tf.math.log(H)+(1-labels)*tf.math.log(1-H))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(logistic_regression(features),labels)\n",
    "    return tape.gradient(loss_value, [W,b])\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.3494\n",
      "Iter: 100, Loss: 0.3440\n",
      "Iter: 200, Loss: 0.3388\n",
      "Iter: 300, Loss: 0.3337\n",
      "Iter: 400, Loss: 0.3287\n",
      "Iter: 500, Loss: 0.3238\n",
      "Iter: 600, Loss: 0.3191\n",
      "Iter: 700, Loss: 0.3145\n",
      "Iter: 800, Loss: 0.3100\n",
      "Iter: 900, Loss: 0.3056\n",
      "Iter: 1000, Loss: 0.3013\n",
      "Testset Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1001\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels  in iter(dataset):\n",
    "        grads = grad(features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
    "        if step % 100 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))\n",
    "test_acc = accuracy_fn(logistic_regression(x_test),y_test)\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "#convert into numpy and float format\n",
    "x_data = np.asarray(x_data, dtype=np.float32)\n",
    "y_data = np.asarray(y_data, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[ 0.40729985, -0.73095536, -0.55600214],\n",
      "       [-0.96231914,  0.26695555,  2.4163544 ],\n",
      "       [-1.3048412 ,  1.2742552 , -0.80840886],\n",
      "       [-0.6584339 , -0.40664473,  0.45623916]], dtype=float32)> <tf.Variable 'bias:0' shape=(1, 3) dtype=float32, numpy=array([[0.5525866 , 0.659154  , 0.07595671]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.random.normal((4, nb_classes)),name = 'weight')\n",
    "b = tf.Variable(tf.random.normal((1,nb_classes)),name='bias')\n",
    "variables = [W,b]\n",
    "print(W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[9.1521977e-04 6.4660877e-02 9.3442386e-01]\n",
      " [6.2857772e-04 9.3066877e-01 6.8702608e-02]\n",
      " [8.5212773e-04 6.6881251e-01 3.3033538e-01]\n",
      " [1.7406224e-05 9.7879457e-01 2.1188013e-02]\n",
      " [7.1684249e-14 1.9571670e-04 9.9980432e-01]\n",
      " [1.0550339e-07 7.9338682e-01 2.0661312e-01]\n",
      " [4.1753309e-13 5.6554205e-03 9.9434459e-01]\n",
      " [2.8519875e-15 2.2398906e-03 9.9776006e-01]], shape=(8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "\n",
    "print(hypothesis(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_db = [[8,2,1,4]]\n",
    "sample_db = np.asarray(sample_db, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.330042, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def cost_fn(X,Y):\n",
    "    logits = hypothesis(X)\n",
    "    cost = -tf.reduce_sum(Y*tf.math.log(logits),axis=1)\n",
    "    cost_mean =  tf.reduce_mean(cost)\n",
    "    \n",
    "    return cost_mean\n",
    "\n",
    "print(cost_fn(x_data,y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)   # 상수형 텐서를 변수형 텐서처럼 바꿔준다.\n",
    "    y= x*x      # 미분하면 2x니까 x에 3을 넣으면 6이 된다는 것을 보여주는 예시\n",
    "    \n",
    "dy_dx = tape.gradient(y,x)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=138, shape=(4, 3), dtype=float32, numpy=\n",
      "array([[-0.24940018,  0.33113652, -0.08173636],\n",
      "       [-1.6245838 , -0.70683086,  2.3314147 ],\n",
      "       [-1.6243193 , -0.15317476,  1.7774941 ],\n",
      "       [-1.6242914 , -0.21173342,  1.8360248 ]], dtype=float32)>, <tf.Tensor: id=137, shape=(1, 3), dtype=float32, numpy=array([[-0.24969831,  0.05555181,  0.19414648]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "def grad_fn(X,Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X,Y)\n",
    "        grads = tape.gradient(loss,variables)\n",
    "        \n",
    "        return grads\n",
    "\n",
    "print(grad_fn(x_data,y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 6.059923\n",
      "Loss at epoch 100: 0.751996\n",
      "Loss at epoch 200: 0.655748\n",
      "Loss at epoch 300: 0.593954\n",
      "Loss at epoch 400: 0.542366\n",
      "Loss at epoch 500: 0.494552\n",
      "Loss at epoch 600: 0.448283\n",
      "Loss at epoch 700: 0.402605\n",
      "Loss at epoch 800: 0.357078\n",
      "Loss at epoch 900: 0.311737\n",
      "Loss at epoch 1000: 0.268408\n",
      "Loss at epoch 1100: 0.239998\n",
      "Loss at epoch 1200: 0.227840\n",
      "Loss at epoch 1300: 0.217095\n",
      "Loss at epoch 1400: 0.207273\n",
      "Loss at epoch 1500: 0.198263\n",
      "Loss at epoch 1600: 0.189970\n",
      "Loss at epoch 1700: 0.182313\n",
      "Loss at epoch 1800: 0.175224\n",
      "Loss at epoch 1900: 0.168643\n",
      "Loss at epoch 2000: 0.162518\n"
     ]
    }
   ],
   "source": [
    "def fit(X,Y, epochs=2000,verbose=100):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X,Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if(i==0) | ((i+1)%verbose ==0):\n",
    "            print(\"Loss at epoch %d: %f\" %(i+1,cost_fn(X,Y).numpy()))\n",
    "            \n",
    "fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('data_04_zoo.csv', delimiter=',', dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 3.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 6.],\n",
       "       [0., 1., 1., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, -1]  # 데이터는 y가 원핫이 아니다 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 7  # y 데이터가 0~6까지의 값으로 구분되어 있다\n",
    "Y_one_hot = tf.one_hot(y_data.astype(np.int32), nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.normal((16,nb_classes)),name='weight')  #17-1\n",
    "b = tf.Variable(tf.random.normal((nb_classes,)),name='bias')\n",
    "variables = [W,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_fn(X):\n",
    "    return tf.matmul(X,W) + b\n",
    "\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(logit_fn(X))\n",
    "\n",
    "def cost_fn(X,Y):\n",
    "    logits = logit_fn(X)\n",
    "    cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                       labels=Y)\n",
    "    cost = tf.reduce_mean(cost_i)\n",
    "    return cost\n",
    "\n",
    "def grad_fn(X,Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X,Y)\n",
    "        grads = tape.gradient(loss,variables)\n",
    "        return grads\n",
    "    \n",
    "def prediction(X,Y):\n",
    "    pred = tf.argmax(hypothesis(X),1)\n",
    "    correct_prediction = tf.equal(pred, tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1 Loss: 0.41749024391174316, Acc: 0.8613861203193665\n",
      "Steps: 100 Loss: 0.4039740562438965, Acc: 0.8613861203193665\n",
      "Steps: 200 Loss: 0.3911203444004059, Acc: 0.8910890817642212\n",
      "Steps: 300 Loss: 0.3789897859096527, Acc: 0.9009901285171509\n",
      "Steps: 400 Loss: 0.36751440167427063, Acc: 0.9009901285171509\n",
      "Steps: 500 Loss: 0.3566357493400574, Acc: 0.9009901285171509\n",
      "Steps: 600 Loss: 0.34630370140075684, Acc: 0.9009901285171509\n",
      "Steps: 700 Loss: 0.3364744186401367, Acc: 0.9009901285171509\n",
      "Steps: 800 Loss: 0.327109694480896, Acc: 0.9009901285171509\n",
      "Steps: 900 Loss: 0.318175733089447, Acc: 0.9009901285171509\n",
      "Steps: 1000 Loss: 0.3096427619457245, Acc: 0.9108911156654358\n"
     ]
    }
   ],
   "source": [
    "def fit(X,Y,epochs=1000,verbose=100):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X,Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "\n",
    "            acc = prediction(X, Y).numpy()\n",
    "            loss = cost_fn(X, Y).numpy() \n",
    "            print('Steps: {} Loss: {}, Acc: {}'.format(i+1, loss, acc))\n",
    "\n",
    "fit(x_data, Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오버피팅 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_train = xy[:, 0:-1]\n",
    "y_train = xy[:, [-1]]\n",
    "\n",
    "plt.plot(x_train, 'ro')\n",
    "plt.plot(y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 크기의 격차가 너무 크기 때문에 정규화를 해주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0) - np.min(data,0)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = normalization(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = xy[:, 0:-1]\n",
    "y_train = xy[:, [-1]]\n",
    "\n",
    "plt.plot(x_train, 'ro')\n",
    "plt.plot(y_train)\n",
    "plt.title(\"nomalization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([4,1]),dtype=tf.float32)\n",
    "b = tf.Variable(tf.random_normal([1]),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearReg_fn(features):\n",
    "    H = tf.matmul(features,W)+b\n",
    "    return H\n",
    "\n",
    "def l2_loss(loss,beta=0.01):\n",
    "    W_reg = tf.nn.l2_loss(W)\n",
    "    loss=tf.reduce_mean(loss + W_reg * beta)\n",
    "    return loss\n",
    "    \n",
    "\n",
    "def loss_fn(H,labels,flag = False):\n",
    "    cost = tf.reduce_mean(tf.square(H-labels))\n",
    "    if(flag):\n",
    "        cost = l2_loss(cost)\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_decay = True\n",
    "starter_learning_rate=0.1\n",
    "\n",
    "if(is_decay):\n",
    "    global_step = tf.Variable(0, trainable = False)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 50, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "else:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=starter_learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(features, labels, l2_flag):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(linearReg_fn(features),labels, l2_flag)\n",
    "    return tape.gradient(loss_value,[W,b]),loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.0122, Learning Rate : 0.10000000\n",
      "Iter: 10, Loss: 0.0121, Learning Rate : 0.10000000\n",
      "Iter: 20, Loss: 0.0120, Learning Rate : 0.10000000\n",
      "Iter: 30, Loss: 0.0119, Learning Rate : 0.10000000\n",
      "Iter: 40, Loss: 0.0117, Learning Rate : 0.10000000\n",
      "Iter: 50, Loss: 0.0116, Learning Rate : 0.09600000\n",
      "Iter: 60, Loss: 0.0115, Learning Rate : 0.09600000\n",
      "Iter: 70, Loss: 0.0114, Learning Rate : 0.09600000\n",
      "Iter: 80, Loss: 0.0113, Learning Rate : 0.09600000\n",
      "Iter: 90, Loss: 0.0112, Learning Rate : 0.09600000\n",
      "Iter: 100, Loss: 0.0111, Learning Rate : 0.09216000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 101\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels in dataset:\n",
    "        features = tf.cast(features, tf.float32)\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "        grads, loss_value = grad(features, labels, False)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]), global_step=global_step)\n",
    "    if step % 10 == 0:\n",
    "        print(\"Iter: {}, Loss: {:2.4f}, Learning Rate : {:2.8f}\".format(step, loss_value, optimizer._learning_rate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist=tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels),(test_images,test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images /255.0\n",
    "test_images = test_images /255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(128,activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 19s 316us/sample - loss: 0.4969 - acc: 0.8257\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 17s 281us/sample - loss: 0.3726 - acc: 0.8664\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 18s 292us/sample - loss: 0.3340 - acc: 0.8776\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.3080 - acc: 0.8865\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.2926 - acc: 0.8924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2429b2df5f8>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images,train_labels,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jw_tensorflow",
   "language": "python",
   "name": "jw_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
